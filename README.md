# Switch

<!-- DIAGRAM:system -->
<!-- (auto-generated by scripts/sync-diagrams.py; edit docs/diagrams/system.mermaid) -->
```mermaid
flowchart LR
    subgraph User["User Devices"]
        Client["XMPP Client<br/>(Conversations, Gajim, etc.)"]
    end

    subgraph Tailnet["Tailscale Network"]
        subgraph DevBox["Development Machine"]
            XMPP["ejabberd<br/>(XMPP Server)"]

            subgraph Orchestrators["Orchestrator Contacts"]
                direction TB
                CC["cc@...<br/>(Claude Code)"]
                OC["oc@...<br/>(OpenCode GLM 4.7)"]
                OCGPT["oc-gpt@...<br/>(OpenCode GPT 5.2)"]
            end

            Sessions["Session Bots<br/>(task-name@...)"]

            subgraph Engines["AI CLIs"]
                direction TB
                OpenCode["OpenCode CLI"]
                Claude["Claude CLI"]
            end
        end
    end

    Client <-->|"Tailscale IP"| XMPP
    XMPP <--> CC
    XMPP <--> OC
    XMPP <--> OCGPT
    XMPP <--> Sessions
    Sessions --> OpenCode
    Sessions --> Claude

    classDef orchestrator fill:#f5f5e8,stroke:#8a7d60,color:#2c2c2c;
    class CC,OC,OCGPT orchestrator;
```
<!-- /DIAGRAM:system -->

Chat with AI coding assistants from any XMPP client. Each conversation becomes a separate contact, making it easy to manage multiple concurrent sessions from your phone or desktop.

Designed to run on a dedicated Linux machine (old laptop, mini PC, home server) so the AI has real system access to do useful work.

Switch is the calm center for a swarm of agents. Spin up multiple sessions at once, let them talk to each other, and keep every thread portable across phone and desktop. Your sessions are not tied to a single provider, so you keep the flexibility of multiple backends while still getting the muscle of agent harnesses. It is simple open source software tied together with the power of SOTA when desired.

## Features

- **Multi-session**: Each conversation is a separate XMPP contact
- **Multiple orchestrators**: Three contacts for different AI backends
- **Mobile-friendly**: Works with any XMPP client (Conversations, Gajim, Dino, etc.)
- **Session persistence**: Resume conversations after restarts
- **Ralph loops**: Autonomous iteration for long-running tasks
- **Shell access**: Run commands directly from chat
- **Local memory vault**: Gitignored notes under `memory/`

## Quick Start

```bash
# Install dependencies
uv sync

# Install git hooks (optional but recommended)
./scripts/install-pre-commit.sh

# Configure
cp .env.example .env
# Edit .env with your XMPP server details

# Agent instructions are symlinked to home (for Claude Code and OpenCode)
# Edit ~/switch/AGENTS.md - symlinks at ~/AGENTS.md and ~/CLAUDE.md point here

# Run
uv run python -m src.bridge
```

## Orchestrator Contacts

Each AI backend has its own XMPP contact. Message any of them to start a session:

| Contact | Backend | Model |
|---------|---------|-------|
| `cc@...` | Claude Code | Claude Opus |
| `oc@...` | OpenCode | GLM 4.7 |
| `oc-gpt@...` | OpenCode | GPT 5.2 |
| `oc-glm-zen@...` | OpenCode | GLM 4.7 (Zen) |
| `oc-gpt-or@...` | OpenCode | GPT 5.2 (OpenRouter) |
| `oc-kimi-coding@...` | OpenCode | Kimi K2.5 (Kimi for Coding) |

Sessions appear as separate contacts (e.g., `fix-auth-bug@...`) so you can have multiple conversations in parallel.

## How It Works

```mermaid
flowchart LR
    You --> Client[XMPP Client]
    subgraph Orchestrators["Orchestrator Contacts"]
        direction TB
        cc["cc@..."]
        oc["oc@..."]
        ocgpt["oc-gpt@..."]
    end

    subgraph Sessions["Session Contacts"]
        direction TB
        s1["fix-auth-bug@..."]
        s2["add-tests@..."]
        s3["refactor-db@..."]
    end

    Client --> cc
    Client --> oc
    Client --> ocgpt
    Client --> Sessions

    cc --> Claude[Claude Code]
    oc --> GLM["OpenCode (GLM 4.7)"]
    ocgpt --> GPT["OpenCode (GPT 5.2)"]

    s1 --> Claude
    s2 --> GLM
    s3 --> Claude
```

## Basic Usage

| Action | Command |
|--------|---------|
| New Claude session | Message `cc@...` |
| New GLM session | Message `oc@...` |
| New GPT session | Message `oc-gpt@...` |
| Cancel current run | `/cancel` |
| Run shell command | `!git status` |
| List sessions | `/list` to any orchestrator |

## Documentation

- [Setup Guide](docs/setup.md) - Hardware, installation, configuration
- [Commands Reference](docs/commands.md) - All available commands
- [Architecture](docs/architecture.md) - How the system works
- [Memory Vault](docs/memory.md) - Store local learnings and runbooks
- [AGENTS.md](AGENTS.md) - Instructions for AI agents working on this codebase

## Requirements

- Dedicated Linux machine (bare metal preferred)
- Python 3.11+
- ejabberd XMPP server
- OpenCode CLI and/or Claude Code CLI
- tmux
- [Tailscale](https://tailscale.com/) (recommended for secure remote access)

## Models

- **cc**: Claude Opus via Claude Code CLI
- **oc**: GLM 4.7 via OpenCode - fast, cheap, good for iteration
- **oc-gpt**: GPT 5.2 via OpenCode - alternative for comparison
- **oc-kimi-coding**: Kimi K2.5 via Kimi for Coding

## License

MIT
